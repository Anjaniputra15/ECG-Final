{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ü´Ä ECG-LLM Training Runner for GitHub Sync\n",
        "\n",
        "This notebook runs your ECG-LLM code from GitHub on Colab GPU.\n",
        "\n",
        "**Workflow:**\n",
        "1. Develop in Windsurf locally\n",
        "2. Push to GitHub\n",
        "3. Run this notebook in Colab for GPU training\n",
        "4. Results auto-save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "# Check GPU setup\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "print(f\"üñ•Ô∏è  CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "print(\"\\nüíæ Available storage:\")\n",
        "!df -h | head -2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install wfdb neurokit2 pandas numpy matplotlib seaborn\n",
        "!pip install opencv-python scikit-learn tqdm\n",
        "!pip install transformers timm efficientnet-pytorch\n",
        "!pip install Pillow\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone your GitHub repository\n",
        "# REPLACE with your actual GitHub repo URL\n",
        "GITHUB_REPO = \"https://github.com/yourusername/ecg-llm.git\"  # CHANGE THIS!\n",
        "\n",
        "import os\n",
        "if os.path.exists('ecg-llm'):\n",
        "    print(\"üìÅ Repository already exists, pulling latest changes...\")\n",
        "    %cd ecg-llm\n",
        "    !git pull\n",
        "else:\n",
        "    print(\"üì• Cloning repository...\")\n",
        "    !git clone {GITHUB_REPO}\n",
        "    %cd ecg-llm\n",
        "\n",
        "print(\"\\nüìÇ Repository contents:\")\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_drive"
      },
      "outputs": [],
      "source": [
        "# Setup Google Drive for results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create results directory\n",
        "results_dir = '/content/drive/MyDrive/ECG_Training_Results'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "os.makedirs(f'{results_dir}/models', exist_ok=True)\n",
        "os.makedirs(f'{results_dir}/experiments', exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Results will be saved to: {results_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "# Download PTB-XL dataset (only run once)\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "data_dir = \"data\"\n",
        "dataset_name = \"ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
        "\n",
        "if not os.path.exists(f\"{data_dir}/{dataset_name}\"):\n",
        "    print(\"üì• Downloading PTB-XL dataset (this takes 5-10 minutes)...\")\n",
        "    \n",
        "    url = \"https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\"\n",
        "    \n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    \n",
        "    # Download\n",
        "    urllib.request.urlretrieve(url, \"ptb-xl.zip\")\n",
        "    print(\"‚úÖ Download complete!\")\n",
        "    \n",
        "    # Extract\n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(\"ptb-xl.zip\", 'r') as zipf:\n",
        "        zipf.extractall(data_dir)\n",
        "    \n",
        "    # Cleanup\n",
        "    os.remove(\"ptb-xl.zip\")\n",
        "    print(\"‚úÖ Dataset ready!\")\n",
        "else:\n",
        "    print(\"‚úÖ PTB-XL dataset already available!\")\n",
        "\n",
        "# Verify dataset\n",
        "dataset_path = f\"{data_dir}/{dataset_name}\"\n",
        "if os.path.exists(f\"{dataset_path}/ptbxl_database.csv\"):\n",
        "    import pandas as pd\n",
        "    db = pd.read_csv(f\"{dataset_path}/ptbxl_database.csv\")\n",
        "    print(f\"üìä Dataset verified: {len(db):,} ECG records\")\nelse:\n",
        "    print(\"‚ùå Dataset verification failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_bootstrap"
      },
      "outputs": [],
      "source": [
        "# Run bootstrap training (Phase 1)\n",
        "print(\"üöÄ Starting Bootstrap R-Peak Training...\")\n",
        "\n",
        "# Your bootstrap trainer adapted for Colab\n",
        "!python bootstrap_trainer.py --device cuda --batch-size 16 --epochs 25 --output-dir {results_dir}/bootstrap\n",
        "\n",
        "print(\"‚úÖ Bootstrap training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_advanced"
      },
      "outputs": [],
      "source": [
        "# Run advanced training (Phase 2)\n",
        "print(\"üöÄ Starting Advanced Multi-Model Training...\")\n",
        "\n",
        "# Your advanced trainer\n",
        "!python training/advanced_trainer.py --model-type ensemble --device cuda --epochs 50 --batch-size 8 --output-dir {results_dir}/advanced\n",
        "\n",
        "print(\"‚úÖ Advanced training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "# Save all results to Google Drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "session_dir = f\"{results_dir}/training_session_{timestamp}\"\n",
        "os.makedirs(session_dir, exist_ok=True)\n",
        "\n",
        "print(\"üíæ Saving results to Google Drive...\")\n",
        "\n",
        "# Save models\n",
        "model_files = [\"best_model.pth\", \"latest_checkpoint.pth\"]\n",
        "for model_file in model_files:\n",
        "    if os.path.exists(model_file):\n",
        "        shutil.copy2(model_file, f\"{session_dir}/{model_file}\")\n",
        "        print(f\"‚úÖ Saved {model_file}\")\n",
        "\n",
        "# Save experiments\n",
        "if os.path.exists(\"experiments\"):\n",
        "    shutil.copytree(\"experiments\", f\"{session_dir}/experiments\", dirs_exist_ok=True)\n",
        "    print(\"‚úÖ Saved experiments\")\n",
        "\n",
        "# Save any plots/results\n",
        "result_patterns = [\"*.png\", \"*.jpg\", \"*.json\", \"training_*.csv\"]\n",
        "import glob\n",
        "for pattern in result_patterns:\n",
        "    files = glob.glob(pattern)\n",
        "    for file in files:\n",
        "        shutil.copy2(file, f\"{session_dir}/{file}\")\n",
        "        print(f\"‚úÖ Saved {file}\")\n",
        "\n",
        "print(f\"\\nüéâ All results saved to: {session_dir}\")\n",
        "print(\"You can access them from Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "# Quick model test\n",
        "print(\"üß™ Testing trained model...\")\n",
        "\n",
        "if os.path.exists(\"test_real_data.py\"):\n",
        "    !python test_real_data.py --model-path best_model.pth --num-samples 5\nelse:\n",
        "    print(\"‚ö†Ô∏è  Test script not found, skipping model test\")\n",
        "\n",
        "print(\"‚úÖ Testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üéØ Training Complete!\n",
        "\n",
        "Your ECG-LLM model has been trained on Google Colab GPU using your latest Windsurf code.\n",
        "\n",
        "**Next Steps:**\n",
        "1. Check Google Drive for your training results\n",
        "2. Download the best model for local testing\n",
        "3. Continue development in Windsurf\n",
        "4. Push updates and re-run this notebook as needed\n",
        "\n",
        "**Perfect Workflow:**\n",
        "- üíª Develop in Windsurf (fast, local)\n",
        "- üöÄ Train on Colab (GPU power)\n",
        "- üóÇÔ∏è Results in Drive (persistent)\n",
        "- üîÑ Repeat and iterate!\n"
      ]
    }
  ]
}