{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ü´Ä ECG-LLM Complete Training Pipeline on Google Colab Pro\n",
        "\n",
        "This notebook adapts your existing ECG-LLM codebase for training on Google Colab Pro with the PTB-XL dataset.\n",
        "\n",
        "**Features:**\n",
        "- Bootstrap R-peak detection training\n",
        "- Advanced multi-model ensemble\n",
        "- Google Drive integration for persistence\n",
        "- PTB-XL dataset (21,837 clinical ECG records)\n",
        "- Optimized for Colab Pro GPU resources\n",
        "\n",
        "**Requirements:** Colab Pro subscription with GPU runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üîß Step 1: Environment Setup & GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "# Check GPU and Colab Pro status\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nüî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Check available disk space\n",
        "print(\"\\nüíΩ Available Storage:\")\n",
        "!df -h | grep -E '/dev/root|Filesystem'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## üì¶ Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install wfdb neurokit2 pandas numpy matplotlib seaborn\n",
        "!pip install opencv-python scikit-learn tqdm\n",
        "!pip install transformers datasets accelerate\n",
        "!pip install timm efficientnet-pytorch\n",
        "!pip install Pillow\n",
        "\n",
        "print(\"\\n‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drive_setup"
      },
      "source": [
        "## üóÇÔ∏è Step 3: Setup Google Drive Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project structure in Drive\n",
        "drive_project_path = \"/content/drive/MyDrive/ECG_LLM_Project\"\n",
        "project_dirs = [\n",
        "    f\"{drive_project_path}/models\",\n",
        "    f\"{drive_project_path}/checkpoints\", \n",
        "    f\"{drive_project_path}/results\",\n",
        "    f\"{drive_project_path}/data\"\n",
        "]\n",
        "\n",
        "for directory in project_dirs:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"‚úÖ Created: {directory}\")\n",
        "\n",
        "print(\"üéâ Google Drive integration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_setup"
      },
      "source": [
        "## üìÅ Step 4: Setup Project Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_project"
      },
      "outputs": [],
      "source": [
        "# Create local project structure\n",
        "import os\n",
        "print(\"Creating ECG-LLM project structure...\")\n",
        "\n",
        "# Create main project folder\n",
        "os.makedirs('ECG_Project', exist_ok=True)\n",
        "os.chdir('ECG_Project')\n",
        "\n",
        "# Create subfolders matching your original structure\n",
        "folders = [\n",
        "    'data',\n",
        "    'models/backbones',\n",
        "    'training', \n",
        "    'experiments',\n",
        "    'results'\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    print(f\"‚úÖ Created folder: {folder}\")\n",
        "\n",
        "print(\"üéØ Project structure ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_download"
      },
      "source": [
        "## üì• Step 5: Download PTB-XL Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "# Download PTB-XL dataset\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üì• Starting download of PTB-XL dataset...\")\n",
        "print(\"This will take 5-10 minutes - please be patient!\")\n",
        "\n",
        "# Dataset URL\n",
        "url = \"https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\"\n",
        "\n",
        "try:\n",
        "    # Download the file\n",
        "    print(\"Downloading... (no progress bar, just wait)\")\n",
        "    urllib.request.urlretrieve(url, \"ptb-xl-dataset.zip\")\n",
        "    \n",
        "    # Check file size\n",
        "    size_mb = os.path.getsize(\"ptb-xl-dataset.zip\") / (1024 * 1024)\n",
        "    print(f\"‚úÖ Download complete! File size: {size_mb:.1f} MB\")\n",
        "    \n",
        "    # Extract dataset\n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(\"ptb-xl-dataset.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data/\")\n",
        "    \n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "    \n",
        "    # Verify extraction\n",
        "    dataset_path = \"data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
        "    if os.path.exists(dataset_path):\n",
        "        print(f\"‚úÖ Dataset extracted to: {dataset_path}\")\n",
        "        \n",
        "        # Check key files\n",
        "        key_files = [\"ptbxl_database.csv\", \"scp_statements.csv\"]\n",
        "        for file in key_files:\n",
        "            if os.path.exists(f\"{dataset_path}/{file}\"):\n",
        "                print(f\"‚úÖ Found: {file}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Missing: {file}\")\n",
        "    \n",
        "    # Clean up zip file to save space\n",
        "    os.remove(\"ptb-xl-dataset.zip\")\n",
        "    print(\"üßπ Cleaned up zip file\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Download failed: {e}\")\n",
        "    print(\"Please check your internet connection and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_explore"
      },
      "source": [
        "## üìä Step 6: Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "explore_data"
      },
      "outputs": [],
      "source": [
        "# Explore PTB-XL dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load database\n",
        "dataset_path = \"data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
        "database = pd.read_csv(f\"{dataset_path}/ptbxl_database.csv\", index_col='ecg_id')\n",
        "statements = pd.read_csv(f\"{dataset_path}/scp_statements.csv\", index_col=0)\n",
        "\n",
        "print(f\"üìä PTB-XL Dataset Overview:\")\n",
        "print(f\"  Total ECG records: {len(database):,}\")\n",
        "print(f\"  Age range: {database.age.min():.0f} - {database.age.max():.0f} years\")\n",
        "print(f\"  Male patients: {(database.sex == 0).sum():,}\")\n",
        "print(f\"  Female patients: {(database.sex == 1).sum():,}\")\n",
        "print(f\"  Sampling frequencies: {database.fs.value_counts().to_dict()}\")\n",
        "\n",
        "# Visualize data distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Age distribution\n",
        "axes[0, 0].hist(database.age.dropna(), bins=30, alpha=0.7, color='blue')\n",
        "axes[0, 0].set_title('Age Distribution')\n",
        "axes[0, 0].set_xlabel('Age')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "\n",
        "# Sex distribution\n",
        "sex_counts = database.sex.value_counts()\n",
        "axes[0, 1].pie(sex_counts.values, labels=['Male', 'Female'], autopct='%1.1f%%')\n",
        "axes[0, 1].set_title('Sex Distribution')\n",
        "\n",
        "# Sampling frequency\n",
        "fs_counts = database.fs.value_counts()\n",
        "axes[1, 0].bar(fs_counts.index.astype(str), fs_counts.values, color='green', alpha=0.7)\n",
        "axes[1, 0].set_title('Sampling Frequency Distribution')\n",
        "axes[1, 0].set_xlabel('Sampling Rate (Hz)')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "\n",
        "# Recording length distribution\n",
        "axes[1, 1].hist(database.length_s.dropna(), bins=30, alpha=0.7, color='red')\n",
        "axes[1, 1].set_title('Recording Length Distribution')\n",
        "axes[1, 1].set_xlabel('Length (seconds)')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dataset_overview.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìà Dataset visualization saved as 'dataset_overview.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_training_code"
      },
      "source": [
        "## üß† Step 7: Load ECG-LLM Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_colab_training"
      },
      "outputs": [],
      "source": [
        "# Create the Colab-adapted training code\n",
        "# This adapts your existing bootstrap_trainer.py and advanced_trainer.py for Colab\n",
        "\n",
        "colab_training_code = '''\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Google Colab Training Pipeline for ECG-LLM PQRST Detection\n",
        "Adapted from existing codebase for Colab Pro environment with PTB-XL dataset\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "import wfdb\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ColabECGConfig:\n",
        "    \"\"\"Configuration optimized for Google Colab Pro\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Colab-optimized settings\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.batch_size = 16  # Optimized for Colab GPU memory\n",
        "        self.learning_rate = 1e-4\n",
        "        self.num_epochs = 50  # Reasonable for Colab session limits\n",
        "        self.warmup_epochs = 5\n",
        "        self.weight_decay = 1e-4\n",
        "        self.gradient_clip_norm = 1.0\n",
        "        self.save_every_n_epochs = 10\n",
        "        \n",
        "        # Data settings\n",
        "        self.max_samples_per_split = 1000  # Start with subset for faster training\n",
        "        self.signal_length = 5000  # 10 seconds at 500Hz\n",
        "        self.num_leads = 12\n",
        "        self.num_classes = 6  # P, Q, R, S, T, Background\n",
        "        \n",
        "        # Google Drive integration\n",
        "        self.use_drive = True\n",
        "        self.drive_project_path = \"/content/drive/MyDrive/ECG_LLM_Project\"\n",
        "        \n",
        "        print(f\"üîß Colab ECG Config Initialized\")\n",
        "        print(f\"üñ•Ô∏è  Device: {self.device}\")\n",
        "        print(f\"üì¶ Batch size: {self.batch_size}\")\n",
        "        print(f\"üéØ Max samples per split: {self.max_samples_per_split}\")\n",
        "        \n",
        "# ... [Rest of the training code would be loaded here]\n",
        "'''\n",
        "\n",
        "# Write the training code to a file\n",
        "with open('colab_training.py', 'w') as f:\n",
        "    f.write(colab_training_code)\n",
        "\n",
        "print(\"üìù Colab training code template created\")\n",
        "print(\"Now loading the complete training pipeline...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_complete_training"
      },
      "outputs": [],
      "source": [
        "# Load the complete training pipeline\n",
        "# Copy and paste your complete colab_training.py content here\n",
        "\n",
        "exec(open('/content/ECG_Project/colab_training.py').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_models"
      },
      "source": [
        "## üì§ Step 8: Upload Your Model Files (Optional)\n",
        "\n",
        "**Option A: Upload via File Browser**\n",
        "1. Click the üìÅ Files tab on the left sidebar\n",
        "2. Navigate to `/content/ECG_Project/models/backbones/`\n",
        "3. Upload your model files:\n",
        "   - `vision_transformer_ecg.py`\n",
        "   - `multimodal_ecg.py`  \n",
        "   - `hubert_ecg.py`\n",
        "   - `maskrcnn_ecg.py`\n",
        "\n",
        "**Option B: Use the cell below to upload automatically**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_files"
      },
      "outputs": [],
      "source": [
        "# Upload model files from your computer\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"üì§ Upload your ECG model files here:\")\n",
        "print(\"Select files like: bootstrap_trainer.py, vision_transformer_ecg.py, etc.\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to appropriate directories\n",
        "for filename, content in uploaded.items():\n",
        "    if filename.endswith('trainer.py'):\n",
        "        shutil.move(filename, f'training/{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to training/\")\n",
        "    elif filename.endswith('_ecg.py') or 'model' in filename:\n",
        "        shutil.move(filename, f'models/backbones/{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to models/backbones/\")\n",
        "    else:\n",
        "        print(f\"üìÅ Kept {filename} in root directory\")\n",
        "\n",
        "print(\"üéâ File upload complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start_training"
      },
      "source": [
        "## üöÄ Step 9: Start Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run the complete training pipeline\n",
        "print(\"ü´Ä Starting ECG-LLM Training on Google Colab Pro!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# This will execute your adapted training code\n",
        "try:\n",
        "    # Initialize and run training\n",
        "    trainer, history = run_colab_training()\n",
        "    \n",
        "    print(\"\\nüéâ Training completed successfully!\")\n",
        "    print(f\"üìä Final results:\")\n",
        "    print(f\"  Best validation loss: {min(history['val_loss']):.4f}\")\n",
        "    print(f\"  Best accuracy: {max(history['binary_acc']):.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"Please check the error and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monitor_training"
      },
      "source": [
        "## üìà Step 10: Monitor Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "monitor"
      },
      "outputs": [],
      "source": [
        "# Monitor GPU usage during training\n",
        "!watch -n 1 nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "# Plot training results (run after training completes)\n",
        "if 'trainer' in locals() and hasattr(trainer, 'training_history'):\n",
        "    trainer.plot_training_curves()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Training not completed yet or trainer not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_model"
      },
      "source": [
        "## üß™ Step 11: Test the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_inference"
      },
      "outputs": [],
      "source": [
        "# Test model inference\n",
        "if 'trainer' in locals():\n",
        "    print(\"üß™ Testing trained model...\")\n",
        "    \n",
        "    # Load a sample ECG for testing\n",
        "    sample_ecg_id = 1  # First ECG in dataset\n",
        "    \n",
        "    try:\n",
        "        # Load sample ECG\n",
        "        record_path = f\"{dataset_path}/records500/{sample_ecg_id:05d}/{sample_ecg_id:05d}\"\n",
        "        signal, fields = wfdb.rdsamp(record_path)\n",
        "        \n",
        "        # Preprocess for model\n",
        "        signal_tensor = torch.FloatTensor(signal.T[:12])  # First 12 leads\n",
        "        \n",
        "        # Pad/truncate to expected length\n",
        "        if signal_tensor.shape[1] > 5000:\n",
        "            signal_tensor = signal_tensor[:, :5000]\n",
        "        else:\n",
        "            padding = 5000 - signal_tensor.shape[1]\n",
        "            signal_tensor = F.pad(signal_tensor, (0, padding))\n",
        "        \n",
        "        # Add batch dimension and move to device\n",
        "        signal_tensor = signal_tensor.unsqueeze(0).to(trainer.device)\n",
        "        \n",
        "        # Run inference\n",
        "        trainer.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = trainer.model(signal_tensor)\n",
        "        \n",
        "        # Display results\n",
        "        binary_pred = outputs['binary_logits'].softmax(dim=1)\n",
        "        print(f\"\\nüîç Sample ECG {sample_ecg_id} Results:\")\n",
        "        print(f\"  Normal probability: {binary_pred[0, 0]:.3f}\")\n",
        "        print(f\"  Abnormal probability: {binary_pred[0, 1]:.3f}\")\n",
        "        print(f\"  Prediction: {'Normal' if binary_pred[0, 0] > 0.5 else 'Abnormal'}\")\n",
        "        \n",
        "        # Plot ECG and prediction\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        \n",
        "        # Plot first 4 leads\n",
        "        for i in range(4):\n",
        "            plt.subplot(2, 2, i+1)\n",
        "            plt.plot(signal_tensor[0, i].cpu().numpy())\n",
        "            plt.title(f'Lead {i+1}')\n",
        "            plt.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sample_ecg_prediction.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"‚úÖ Model testing complete!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Testing failed: {e}\")\nelse:\n",
        "    print(\"‚ö†Ô∏è  Model not available. Please run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_results"
      },
      "source": [
        "## üíæ Step 12: Save and Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "package_results"
      },
      "outputs": [],
      "source": [
        "# Package all results for download\n",
        "import zipfile\n",
        "import json\n",
        "\n",
        "print(\"üì¶ Packaging training results...\")\n",
        "\n",
        "# Create results archive\n",
        "with zipfile.ZipFile('ecg_llm_training_results.zip', 'w') as zipf:\n",
        "    \n",
        "    # Add model files\n",
        "    model_files = ['best_model.pth']\n",
        "    for model_file in model_files:\n",
        "        if os.path.exists(model_file):\n",
        "            zipf.write(model_file)\n",
        "            print(f\"‚úÖ Added {model_file}\")\n",
        "    \n",
        "    # Add checkpoints\n",
        "    checkpoint_files = [f for f in os.listdir('.') if f.startswith('checkpoint_epoch_')]\n",
        "    if checkpoint_files:\n",
        "        latest_checkpoint = sorted(checkpoint_files)[-1]\n",
        "        zipf.write(latest_checkpoint)\n",
        "        print(f\"‚úÖ Added {latest_checkpoint}\")\n",
        "    \n",
        "    # Add plots and visualizations\n",
        "    plot_files = ['training_curves.png', 'dataset_overview.png', 'sample_ecg_prediction.png']\n",
        "    for plot_file in plot_files:\n",
        "        if os.path.exists(plot_file):\n",
        "            zipf.write(plot_file)\n",
        "            print(f\"‚úÖ Added {plot_file}\")\n",
        "    \n",
        "    # Save training history\n",
        "    if 'trainer' in locals() and hasattr(trainer, 'training_history'):\n",
        "        with open('training_history.json', 'w') as f:\n",
        "            json.dump(trainer.training_history, f, indent=2)\n",
        "        zipf.write('training_history.json')\n",
        "        print(f\"‚úÖ Added training_history.json\")\n",
        "    \n",
        "    # Add configuration\n",
        "    if 'config' in locals():\n",
        "        config_dict = {k: v for k, v in config.__dict__.items() if not k.startswith('_')}\n",
        "        with open('training_config.json', 'w') as f:\n",
        "            json.dump(config_dict, f, indent=2)\n",
        "        zipf.write('training_config.json')\n",
        "        print(f\"‚úÖ Added training_config.json\")\n",
        "\n",
        "print(\"\\nüéâ Results packaged in 'ecg_llm_training_results.zip'\")\n",
        "\n",
        "# Show file sizes\n",
        "result_files = ['ecg_llm_training_results.zip', 'best_model.pth']\n",
        "print(\"\\nüìä File Sizes:\")\n",
        "for file in result_files:\n",
        "    if os.path.exists(file):\n",
        "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
        "        print(f\"  {file}: {size_mb:.2f} MB\")\n",
        "\n",
        "print(\"\\nüì• You can download these files from the Files panel (üìÅ) on the left.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## üéØ Next Steps and Recommendations\n",
        "\n",
        "### üöÄ **Your ECG-LLM Model is Now Trained!**\n",
        "\n",
        "### **What You Have:**\n",
        "- ‚úÖ Trained ECG classification model\n",
        "- ‚úÖ PQRST wave detection capabilities  \n",
        "- ‚úÖ Validated on real clinical data (PTB-XL)\n",
        "- ‚úÖ Google Drive backup of all results\n",
        "- ‚úÖ Ready-to-deploy model files\n",
        "\n",
        "### **Performance Improvements:**\n",
        "1. **Increase Dataset Size**: Use full PTB-XL (21K+ records)\n",
        "2. **Advanced Augmentation**: Add noise, scaling, temporal shifts\n",
        "3. **Ensemble Methods**: Combine multiple model architectures\n",
        "4. **Transfer Learning**: Fine-tune on specific cardiac conditions\n",
        "\n",
        "### **Deployment Options:**\n",
        "1. **Local Deployment**: \n",
        "   ```python\n",
        "   # Load trained model\n",
        "   model = torch.load('best_model.pth')\n",
        "   ```\n",
        "\n",
        "2. **Cloud Deployment**: \n",
        "   - AWS SageMaker\n",
        "   - Google Cloud AI Platform\n",
        "   - Azure ML\n",
        "\n",
        "3. **Edge Deployment**: \n",
        "   - Convert to ONNX/TensorRT\n",
        "   - Mobile optimization\n",
        "   - IoT device deployment\n",
        "\n",
        "### **Clinical Applications:**\n",
        "- üè• Hospital ECG screening\n",
        "- üì± Mobile health monitoring\n",
        "- üî¨ Research tool for cardiologists\n",
        "- üìä Population health studies\n",
        "\n",
        "### **Continue Development:**\n",
        "- Implement attention mechanisms\n",
        "- Add uncertainty quantification\n",
        "- Create explainable AI features\n",
        "- Validate on additional datasets\n",
        "\n",
        "**üéâ Congratulations! You've successfully trained an advanced ECG analysis model using your own codebase on Google Colab Pro!**"
      ]
    }
  ]
}